{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 3 Minimizing Cost\n",
    "# This is optional\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set wrong model weights\n",
    "W = tf.Variable(5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model\n",
    "hypothesis = X * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual gradient\n",
    "gradient = tf.reduce_mean((W * X - Y) * X) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize: Gradient Descent Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gradients\n",
    "gvs = optimizer.compute_gradients(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: modify gradient if necessary\n",
    "# gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "\n",
    "# Apply gradients\n",
    "apply_gradients = optimizer.apply_gradients(gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37.333332 [(37.333336, 4.6266665)]\n",
      "1 33.84889 [(33.84889, 4.2881775)]\n",
      "2 30.689657 [(30.689657, 3.9812808)]\n",
      "3 27.825287 [(27.825287, 3.703028)]\n",
      "4 25.228262 [(25.228262, 3.4507453)]\n",
      "5 22.873621 [(22.873623, 3.2220092)]\n",
      "6 20.738752 [(20.73875, 3.0146217)]\n",
      "7 18.803137 [(18.803137, 2.8265903)]\n",
      "8 17.048176 [(17.048176, 2.6561086)]\n",
      "9 15.457013 [(15.457014, 2.5015385)]\n",
      "10 14.014359 [(14.01436, 2.361395)]\n",
      "11 12.706352 [(12.706352, 2.2343314)]\n",
      "12 11.520427 [(11.520427, 2.119127)]\n",
      "13 10.445186 [(10.445185, 2.0146751)]\n",
      "14 9.470302 [(9.470302, 1.9199722)]\n",
      "15 8.586407 [(8.586407, 1.8341081)]\n",
      "16 7.785009 [(7.785009, 1.756258)]\n",
      "17 7.0584083 [(7.0584083, 1.685674)]\n",
      "18 6.399624 [(6.399624, 1.6216778)]\n",
      "19 5.8023257 [(5.8023252, 1.5636545)]\n",
      "20 5.260776 [(5.260776, 1.5110468)]\n",
      "21 4.7697697 [(4.7697697, 1.4633491)]\n",
      "22 4.324591 [(4.324591, 1.4201032)]\n",
      "23 3.9209633 [(3.9209635, 1.3808936)]\n",
      "24 3.5550067 [(3.5550067, 1.3453435)]\n",
      "25 3.2232056 [(3.2232056, 1.3131114)]\n",
      "26 2.9223735 [(2.9223735, 1.2838877)]\n",
      "27 2.6496189 [(2.6496186, 1.2573916)]\n",
      "28 2.4023216 [(2.4023216, 1.2333684)]\n",
      "29 2.178105 [(2.178105, 1.2115873)]\n",
      "30 1.9748148 [(1.9748147, 1.1918392)]\n",
      "31 1.7904993 [(1.7904994, 1.1739342)]\n",
      "32 1.623386 [(1.6233861, 1.1577003)]\n",
      "33 1.4718695 [(1.4718695, 1.1429816)]\n",
      "34 1.3344955 [(1.3344957, 1.1296366)]\n",
      "35 1.2099417 [(1.2099419, 1.1175373)]\n",
      "36 1.0970144 [(1.0970144, 1.1065671)]\n",
      "37 0.9946267 [(0.9946267, 1.0966209)]\n",
      "38 0.90179497 [(0.901795, 1.087603)]\n",
      "39 0.8176275 [(0.81762755, 1.0794266)]\n",
      "40 0.7413151 [(0.7413151, 1.0720135)]\n",
      "41 0.67212623 [(0.67212623, 1.0652922)]\n",
      "42 0.609394 [(0.609394, 1.0591983)]\n",
      "43 0.5525169 [(0.5525169, 1.0536731)]\n",
      "44 0.50094914 [(0.50094914, 1.0486636)]\n",
      "45 0.45419374 [(0.45419377, 1.0441216)]\n",
      "46 0.41180158 [(0.41180158, 1.0400037)]\n",
      "47 0.37336722 [(0.37336725, 1.03627)]\n",
      "48 0.33851996 [(0.33852, 1.0328848)]\n",
      "49 0.30692515 [(0.30692515, 1.0298156)]\n",
      "50 0.27827826 [(0.2782783, 1.0270327)]\n",
      "51 0.25230527 [(0.25230527, 1.0245097)]\n",
      "52 0.2287569 [(0.2287569, 1.022222)]\n",
      "53 0.20740573 [(0.20740573, 1.020148)]\n",
      "54 0.18804836 [(0.18804836, 1.0182675)]\n",
      "55 0.17049654 [(0.17049655, 1.0165626)]\n",
      "56 0.15458433 [(0.15458433, 1.0150168)]\n",
      "57 0.14015675 [(0.14015675, 1.0136153)]\n",
      "58 0.12707591 [(0.12707591, 1.0123445)]\n",
      "59 0.11521538 [(0.11521538, 1.0111923)]\n",
      "60 0.10446167 [(0.10446167, 1.0101477)]\n",
      "61 0.09471202 [(0.09471202, 1.0092006)]\n",
      "62 0.08587202 [(0.08587202, 1.0083419)]\n",
      "63 0.07785805 [(0.07785805, 1.0075634)]\n",
      "64 0.07059129 [(0.07059129, 1.0068574)]\n",
      "65 0.06400236 [(0.06400236, 1.0062174)]\n",
      "66 0.05802846 [(0.05802846, 1.005637)]\n",
      "67 0.052612226 [(0.052612226, 1.005111)]\n",
      "68 0.047702473 [(0.047702473, 1.0046339)]\n",
      "69 0.043249767 [(0.043249767, 1.0042014)]\n",
      "70 0.03921318 [(0.03921318, 1.0038093)]\n",
      "71 0.035553534 [(0.035553537, 1.0034539)]\n",
      "72 0.032236177 [(0.03223618, 1.0031315)]\n",
      "73 0.029227654 [(0.029227655, 1.0028392)]\n",
      "74 0.02649951 [(0.02649951, 1.0025742)]\n",
      "75 0.024025917 [(0.024025917, 1.002334)]\n",
      "76 0.021783749 [(0.02178375, 1.0021162)]\n",
      "77 0.01975123 [(0.019751232, 1.0019187)]\n",
      "78 0.017907381 [(0.017907381, 1.0017396)]\n",
      "79 0.016236702 [(0.016236704, 1.0015773)]\n",
      "80 0.014720838 [(0.014720838, 1.00143)]\n",
      "81 0.01334699 [(0.013346991, 1.0012965)]\n",
      "82 0.012100856 [(0.012100856, 1.0011755)]\n",
      "83 0.010971785 [(0.010971785, 1.0010659)]\n",
      "84 0.0099481745 [(0.0099481745, 1.0009663)]\n",
      "85 0.009018898 [(0.009018898, 1.0008761)]\n",
      "86 0.008176883 [(0.008176884, 1.0007943)]\n",
      "87 0.007413149 [(0.007413149, 1.0007201)]\n",
      "88 0.006721576 [(0.006721576, 1.0006529)]\n",
      "89 0.0060940585 [(0.0060940585, 1.000592)]\n",
      "90 0.005525271 [(0.0055252714, 1.0005368)]\n",
      "91 0.0050098896 [(0.0050098896, 1.0004867)]\n",
      "92 0.004542589 [(0.004542589, 1.0004413)]\n",
      "93 0.0041189194 [(0.0041189194, 1.0004001)]\n",
      "94 0.0037339528 [(0.003733953, 1.0003628)]\n",
      "95 0.0033854644 [(0.0033854644, 1.0003289)]\n",
      "96 0.0030694802 [(0.0030694804, 1.0002983)]\n",
      "97 0.0027837753 [(0.0027837753, 1.0002704)]\n",
      "98 0.0025234222 [(0.0025234222, 1.0002451)]\n",
      "99 0.0022875469 [(0.0022875469, 1.0002222)]\n",
      "100 0.0020739238 [(0.0020739238, 1.0002015)]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        gradient_val, gvs_val, _ = sess.run([gradient, gvs, apply_gradients])\n",
    "        print(step, gradient_val, gvs_val)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
