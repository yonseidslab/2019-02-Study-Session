library(ggplot2)
library(cowplot)
## NOTE: https://github.com/StatQuest/logistic_regression_demo/blob/master/logistic_regression_demo.R
 
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"
 
df <- read.csv(url, header=FALSE)
 
#####################################
##
## Reformat the data so that it is
## 1) Easy to use (add nice column names)
## 2) Interpreted correctly by glm()..
##
##################################### 
colnames(df) <- c(
  "age",
  "sex",# 0 = female, 1 = male
  "cp", # chest pain
  # 1 = typical angina,
  # 2 = atypical angina,
  # 3 = non-anginal pain,
  # 4 = asymptomatic
  "trestbps", # resting blood pressure (in mm Hg)
  "chol", # serum cholestoral in mg/dl
  "fbs",  # fasting blood sugar if less than 120 mg/dl, 1 = TRUE, 0 = FALSE
  "restecg", # resting electrocardiographic results
  # 1 = normal
  # 2 = having ST-T wave abnormality
  # 3 = showing probable or definite left ventricular hypertrophy
  "thalach", # maximum heart rate achieved
  "exang",   # exercise induced angina, 1 = yes, 0 = no
  "oldpeak", # ST depression induced by exercise relative to rest
  "slope", # the slope of the peak exercise ST segment
  # 1 = upsloping
  # 2 = flat
  # 3 = downsloping
  "ca", # number of major vessels (0-3) colored by fluoroscopy
  "thal", # this is short of thalium heart scan
  # 3 = normal (no cold spots)
  # 6 = fixed defect (cold spots during rest and exercise)
  # 7 = reversible defect (when cold spots only appear during exercise)
  "hd" # (the predicted attribute) - diagnosis of heart disease
  # 0 if less than or equal to 50% diameter narrowing
  # 1 if greater than 50% diameter narrowing
)
 
head(df) # now we have data and column names
 
str(df) # this shows that we need to tell R which columns contain factors
# it also shows us that there are some missing values. There are "?"s
# in the dataset. These are in the "ca" and "thal" columns...
 
## First, convert "?"s to NAs...
df[df == "?"] <- NA
data = df

##df는 linear model을 이용하기 위해 나중에 다시 쓸 것임.
## Now add factors for variables that are factors and clean up the factors
## that had missing data...
data[data$sex == 0,]$sex <- "F"
data[data$sex == 1,]$sex <- "M"

data$sex <- as.factor(data$sex)
data$cp <- as.factor(data$cp)
data$fbs <- as.factor(data$fbs)
data$restecg <- as.factor(data$restecg)
data$exang <- as.factor(data$exang)
data$slope <- as.factor(data$slope)
 
data$ca <- as.integer(data$ca) # since this column had "?"s in it
data$ca <- as.factor(data$ca)  # ...then convert the integers to factor levels
 
data$thal <- as.integer(data$thal) # "thal" also had "?"s in it.
data$thal <- as.factor(data$thal)
 
## This next line replaces 0 and 1 with "Healthy" and "Unhealthy"
data$hd <- ifelse(test=data$hd == 0, yes="Healthy", no="Unhealthy")
data$hd <- as.factor(data$hd) # Now convert to a factor
 
str(data) ## this shows that the correct columns are factors
 
## Now determine how many rows have "NA" (aka "Missing data"). If it's just
## a few, we can remove them from the dataset, otherwise we should consider
## imputing the values with a Random Forest or some other imputation method.
nrow(data[is.na(data$ca) | is.na(data$thal),])
data[is.na(data$ca) | is.na(data$thal),]
## so 6 of the 303 rows of data have missing values. This isn't a large
## percentage (2%), so we can just remove them from the dataset
## NOTE: This is different from when we did machine learning with
## Random Forests. When we did that, we imputed values.
nrow(data)
data <- data[!(is.na(data$ca) | is.na(data$thal)),]
nrow(data)
 
logistic <- glm(hd ~ ., data=data, family="binomial")
summary(logistic)
 
predicted.data <- data.frame(
  probability.of.hd=logistic$fitted.values,
  hd=data$hd)

predicted.data <- predicted.data[
  order(predicted.data$probability.of.hd, decreasing=FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
 
## Lastly, we can plot the predicted probabilities for each sample having
## heart disease and color by whether or not they actually had heart disease
ggplot(data=predicted.data, aes(x=rank, y=probability.of.hd)) +
  geom_point(aes(color=hd), alpha=1, shape=4, stroke=2) +
  xlab("Index") +
  ylab("Predicted probability of getting heart disease")

ggsave("heart_disease_probabilities.pdf")

tmp = predicted.data[which(predicted.data$probability.of.hd >= 0.5),]
100*(length(which(tmp$hd == "Healthy"))/nrow(tmp))
#healthy인데 unhealthy로 판명 : 10.93%

tmp = predicted.data[which(predicted.data$probability.of.hd < 0.5),]
100*(length(which(tmp$hd == "Unhealthy"))/nrow(tmp))
#unhealthy인데 healthy로 판명 : 14.28%

##########################################################################################
##########################################################################################

df$hd <- ifelse(test=df$hd == 0, yes=0, no=1)
df$ca <- as.integer(df$ca) # since this column had "?"s in it
df$thal <- as.integer(df$thal) # "thal" also had "?"s in it.
df <- df[!(is.na(df$ca) | is.na(df$thal)),]

linear <- lm(hd ~ ., data=df)
summary(linear)
 
predicted.df <- data.frame(
  probability.of.hd=linear$fitted.values,
  hd=df$hd)

predicted.df <- predicted.df[
  order(predicted.df$probability.of.hd, decreasing=FALSE),]
predicted.df$rank <- 1:nrow(predicted.df)

predicted.df$hd <- ifelse(test=predicted.df$hd == 0, yes="Healthy", no="Unhealthy")

ggplot(data=predicted.df, aes(x=rank, y=probability.of.hd)) +
  geom_point(aes(color=hd), alpha=1, shape=4, stroke=2) +
  xlab("Index") +
  ylab("Predicted probability of getting heart disease")

ggsave("heart_disease_probabilities_linear_model.pdf")

tmp = predicted.df[which(predicted.df$probability.of.hd >= 0.5),]
100*(length(which(tmp$hd == "Healthy"))/nrow(tmp))
#healthy인데 unhealthy로 판명 : 13.95%

tmp = predicted.df[which(predicted.df$probability.of.hd < 0.5),]
100*(length(which(tmp$hd == "Unhealthy"))/nrow(tmp))
#unhealthy인데 healthy로 판명 : 16.09%

##########################################################################################
##########################################################################################

linear <- lm(hd ~ ., data=df)
tmp_md <- glm(hd ~ ., data=df, family = "binomial")

drops <- c("hd")
newdata <- df[ , !(names(df) %in% drops)]

before <- predict(tmp_md,newdata=newdata,type=c("link"))
after <- predict(tmp_md, newdata=newdata, type = "response")
P = tmp_md$fitted.values